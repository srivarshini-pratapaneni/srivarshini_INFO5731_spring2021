{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srivarshini-pratapaneni/srivarshini_INFO5731_spring2021/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYRXMiz3oThu",
        "outputId": "028009bd-ffea-4291-9c79-59e69b6e6970"
      },
      "source": [
        "#1.2\r\n",
        "import pandas as pd\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "import nltk\r\n",
        "import textblob\r\n",
        "from textblob import TextBlob\r\n",
        "from textblob import Word\r\n",
        "import csv\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "stop = stopwords.words('english')\r\n",
        "case= pd.read_csv(r'/test.txt',error_bad_lines=False, names=[\"tweet\"])\r\n",
        "\r\n",
        "\r\n",
        "print('\\n\\nDATA\\n\\n')\r\n",
        "print(case)\r\n",
        "\r\n",
        "\r\n",
        "case['word_count'] = case['tweet'].apply(lambda x: len(str(x).split(\" \")))\r\n",
        "\r\n",
        "#text with word count\r\n",
        "print('\\n\\nWORD COUNT\\n\\n')\r\n",
        "print(case[['tweet','word_count']])\r\n",
        "\r\n",
        "\r\n",
        "# number of characters\r\n",
        "print('\\n\\nNUMBER OF CHARACTERS\\n\\n')\r\n",
        "case['char_count'] = case['tweet'].str.len() ## this also includes spaces\r\n",
        "print(case[['tweet','char_count']])\r\n",
        "\r\n",
        "def avg_word(sentence):\r\n",
        "  words = sentence.split()\r\n",
        "  return (sum(len(word) for word in words)/len(words))\r\n",
        "\r\n",
        "print('\\n\\nAVERAGE WORD LENGTH\\n\\n')\r\n",
        "\r\n",
        "case['avg_word'] = case['tweet'].apply(lambda x: avg_word(x))\r\n",
        "print(case[['tweet','avg_word']])\r\n",
        "\r\n",
        "\r\n",
        "print('\\n\\nNUMBER OF STOP WORDS\\n\\n')\r\n",
        "\r\n",
        "stop = stopwords.words('english')\r\n",
        "\r\n",
        "case['stopwords'] = case['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\r\n",
        "print(case[['tweet','stopwords']])\r\n",
        "\r\n",
        "print('\\n\\nNUMBER OF SPECIAL CHARACTERS\\n\\n')\r\n",
        "case['hastags'] = case['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\r\n",
        "print(case[['tweet','hastags']])\r\n",
        "\r\n",
        "print('\\n\\nNUMBER OF NUMERICS\\n\\n')\r\n",
        "case['numerics'] = case['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\r\n",
        "print(case[['tweet','numerics']])\r\n",
        "print('\\n\\nNUMBER OF UPPERCASE\\n\\n')\r\n",
        "case['upper'] = case['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\r\n",
        "print(case[['tweet','upper']])\r\n",
        "print('\\n\\nLower casing\\n\\n')\r\n",
        "case['tweet'] = case['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\nPunctuation removal\\n\\n')\r\n",
        "case['tweet'] = case['tweet'].str.replace('[^\\w\\s]','')\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\nStopwords removal\\n\\n')\r\n",
        "case['tweet'] = case['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\nFrequent words removal\\n\\n')\r\n",
        "freq = pd.Series(' '.join(case['tweet']).split()).value_counts()[-10:]\r\n",
        "freq = list(freq.index)\r\n",
        "case['tweet'] = case['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\nRare words removal\\n\\n')\r\n",
        "freq = pd.Series(' '.join(case['tweet']).split()).value_counts()[-10:]\r\n",
        "freq = list(freq.index)\r\n",
        "case['tweet'] = case['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\r\n",
        "print(case['tweet'])\r\n",
        "print(\"\\n\\nSpelling correction\\n\\n\")\r\n",
        "case['tweet']=case['tweet'].apply(lambda x: str(TextBlob(x).correct()))\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\nTokenization\\n\\n')\r\n",
        "case['tweet']=case['tweet'].apply(lambda x: str(TextBlob(x).words))\r\n",
        "print(case['tweet'])\r\n",
        "print('\\n\\n Stemming\\n\\n')\r\n",
        "st = PorterStemmer()\r\n",
        "case['tweet']=case['tweet'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "print(case['tweet'])\r\n",
        "print(\"\\n\\n Lemmetization\\n\\n\")\r\n",
        "case['tweet'] = case['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "print(case['tweet'])\r\n",
        "\r\n",
        "pd.DataFrame(case['tweet']).to_csv(\"/content/output.csv\")\r\n",
        "print(\"\\n\\nThe data is saved into output.csv file\")\r\n"
      ],
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "\n",
            "\n",
            "DATA\n",
            "\n",
            "\n",
            "                                                 tweet\n",
            "0                                           5 Ala. 740\n",
            "1                            Supreme Court of Alabama.\n",
            "2                                                ADAMS\n",
            "3                                                   v.\n",
            "4                                   TANNER AND HORTON.\n",
            "..                                                 ...\n",
            "142            There are no Filings for this citation.\n",
            "143                                 Negative Treatment\n",
            "144  There are no Negative Treatment results for th...\n",
            "145                                            History\n",
            "146  There are no History results for this citation...\n",
            "\n",
            "[147 rows x 1 columns]\n",
            "\n",
            "\n",
            "WORD COUNT\n",
            "\n",
            "\n",
            "                                                 tweet  word_count\n",
            "0                                           5 Ala. 740           3\n",
            "1                            Supreme Court of Alabama.           4\n",
            "2                                                ADAMS           1\n",
            "3                                                   v.           1\n",
            "4                                   TANNER AND HORTON.           3\n",
            "..                                                 ...         ...\n",
            "142            There are no Filings for this citation.           7\n",
            "143                                 Negative Treatment           2\n",
            "144  There are no Negative Treatment results for th...           9\n",
            "145                                            History           1\n",
            "146  There are no History results for this citation...          21\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "NUMBER OF CHARACTERS\n",
            "\n",
            "\n",
            "                                                 tweet  char_count\n",
            "0                                           5 Ala. 740          10\n",
            "1                            Supreme Court of Alabama.          25\n",
            "2                                                ADAMS           5\n",
            "3                                                   v.           2\n",
            "4                                   TANNER AND HORTON.          18\n",
            "..                                                 ...         ...\n",
            "142            There are no Filings for this citation.          39\n",
            "143                                 Negative Treatment          18\n",
            "144  There are no Negative Treatment results for th...          58\n",
            "145                                            History           7\n",
            "146  There are no History results for this citation...         119\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "AVERAGE WORD LENGTH\n",
            "\n",
            "\n",
            "                                                 tweet  avg_word\n",
            "0                                           5 Ala. 740  2.666667\n",
            "1                            Supreme Court of Alabama.  5.500000\n",
            "2                                                ADAMS  5.000000\n",
            "3                                                   v.  2.000000\n",
            "4                                   TANNER AND HORTON.  5.333333\n",
            "..                                                 ...       ...\n",
            "142            There are no Filings for this citation.  4.714286\n",
            "143                                 Negative Treatment  8.500000\n",
            "144  There are no Negative Treatment results for th...  5.555556\n",
            "145                                            History  7.000000\n",
            "146  There are no History results for this citation...  4.714286\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "NUMBER OF STOP WORDS\n",
            "\n",
            "\n",
            "                                                 tweet  stopwords\n",
            "0                                           5 Ala. 740          0\n",
            "1                            Supreme Court of Alabama.          1\n",
            "2                                                ADAMS          0\n",
            "3                                                   v.          0\n",
            "4                                   TANNER AND HORTON.          0\n",
            "..                                                 ...        ...\n",
            "142            There are no Filings for this citation.          4\n",
            "143                                 Negative Treatment          0\n",
            "144  There are no Negative Treatment results for th...          4\n",
            "145                                            History          0\n",
            "146  There are no History results for this citation...          4\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "NUMBER OF SPECIAL CHARACTERS\n",
            "\n",
            "\n",
            "                                                 tweet  hastags\n",
            "0                                           5 Ala. 740        0\n",
            "1                            Supreme Court of Alabama.        0\n",
            "2                                                ADAMS        0\n",
            "3                                                   v.        0\n",
            "4                                   TANNER AND HORTON.        0\n",
            "..                                                 ...      ...\n",
            "142            There are no Filings for this citation.        0\n",
            "143                                 Negative Treatment        0\n",
            "144  There are no Negative Treatment results for th...        0\n",
            "145                                            History        0\n",
            "146  There are no History results for this citation...        0\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "NUMBER OF NUMERICS\n",
            "\n",
            "\n",
            "                                                 tweet  numerics\n",
            "0                                           5 Ala. 740         2\n",
            "1                            Supreme Court of Alabama.         0\n",
            "2                                                ADAMS         0\n",
            "3                                                   v.         0\n",
            "4                                   TANNER AND HORTON.         0\n",
            "..                                                 ...       ...\n",
            "142            There are no Filings for this citation.         0\n",
            "143                                 Negative Treatment         0\n",
            "144  There are no Negative Treatment results for th...         0\n",
            "145                                            History         0\n",
            "146  There are no History results for this citation...         2\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "NUMBER OF UPPERCASE\n",
            "\n",
            "\n",
            "                                                 tweet  upper\n",
            "0                                           5 Ala. 740      0\n",
            "1                            Supreme Court of Alabama.      0\n",
            "2                                                ADAMS      1\n",
            "3                                                   v.      0\n",
            "4                                   TANNER AND HORTON.      3\n",
            "..                                                 ...    ...\n",
            "142            There are no Filings for this citation.      0\n",
            "143                                 Negative Treatment      0\n",
            "144  There are no Negative Treatment results for th...      0\n",
            "145                                            History      0\n",
            "146  There are no History results for this citation...      4\n",
            "\n",
            "[147 rows x 2 columns]\n",
            "\n",
            "\n",
            "Lower casing\n",
            "\n",
            "\n",
            "0                                             5 ala. 740\n",
            "1                              supreme court of alabama.\n",
            "2                                                  adams\n",
            "3                                                     v.\n",
            "4                                     tanner and horton.\n",
            "                             ...                        \n",
            "142              there are no filings for this citation.\n",
            "143                                   negative treatment\n",
            "144    there are no negative treatment results for th...\n",
            "145                                              history\n",
            "146    there are no history results for this citation...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Punctuation removal\n",
            "\n",
            "\n",
            "0                                              5 ala 740\n",
            "1                               supreme court of alabama\n",
            "2                                                  adams\n",
            "3                                                      v\n",
            "4                                      tanner and horton\n",
            "                             ...                        \n",
            "142               there are no filings for this citation\n",
            "143                                   negative treatment\n",
            "144    there are no negative treatment results for th...\n",
            "145                                              history\n",
            "146    there are no history results for this citation...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Stopwords removal\n",
            "\n",
            "\n",
            "0                                              5 ala 740\n",
            "1                                  supreme court alabama\n",
            "2                                                  adams\n",
            "3                                                      v\n",
            "4                                          tanner horton\n",
            "                             ...                        \n",
            "142                                     filings citation\n",
            "143                                   negative treatment\n",
            "144                  negative treatment results citation\n",
            "145                                              history\n",
            "146    history results citation 5 ala 740 supreme cou...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Frequent words removal\n",
            "\n",
            "\n",
            "0                                              5 ala 740\n",
            "1                                  supreme court alabama\n",
            "2                                                  adams\n",
            "3                                                      v\n",
            "4                                          tanner horton\n",
            "                             ...                        \n",
            "142                                     filings citation\n",
            "143                                   negative treatment\n",
            "144                  negative treatment results citation\n",
            "145                                              history\n",
            "146    history results citation 5 ala 740 supreme cou...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Rare words removal\n",
            "\n",
            "\n",
            "0                                              5 ala 740\n",
            "1                                  supreme court alabama\n",
            "2                                                  adams\n",
            "3                                                      v\n",
            "4                                          tanner horton\n",
            "                             ...                        \n",
            "142                                     filings citation\n",
            "143                                   negative treatment\n",
            "144                  negative treatment results citation\n",
            "145                                              history\n",
            "146    history results citation 5 ala 740 supreme cou...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Spelling correction\n",
            "\n",
            "\n",
            "0                                              5 all 740\n",
            "1                                  supreme court alabama\n",
            "2                                                  adams\n",
            "3                                                      v\n",
            "4                                          manner norton\n",
            "                             ...                        \n",
            "142                                     filing situation\n",
            "143                                   negative treatment\n",
            "144                 negative treatment results situation\n",
            "145                                              history\n",
            "146    history results situation 5 all 740 supreme co...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "Tokenization\n",
            "\n",
            "\n",
            "0                                    ['5', 'all', '740']\n",
            "1                        ['supreme', 'court', 'alabama']\n",
            "2                                              ['adams']\n",
            "3                                                  ['v']\n",
            "4                                   ['manner', 'norton']\n",
            "                             ...                        \n",
            "142                              ['filing', 'situation']\n",
            "143                            ['negative', 'treatment']\n",
            "144    ['negative', 'treatment', 'results', 'situation']\n",
            "145                                          ['history']\n",
            "146    ['history', 'results', 'situation', '5', 'all'...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            " Stemming\n",
            "\n",
            "\n",
            "0                                    ['5', 'all', '740']\n",
            "1                        ['supreme', 'court', 'alabama']\n",
            "2                                              ['adams']\n",
            "3                                                  ['v']\n",
            "4                                   ['manner', 'norton']\n",
            "                             ...                        \n",
            "142                              ['filing', 'situation']\n",
            "143                            ['negative', 'treatment']\n",
            "144    ['negative', 'treatment', 'results', 'situation']\n",
            "145                                          ['history']\n",
            "146    ['history', 'results', 'situation', '5', 'all'...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            " Lemmetization\n",
            "\n",
            "\n",
            "0                                    ['5', 'all', '740']\n",
            "1                        ['supreme', 'court', 'alabama']\n",
            "2                                              ['adams']\n",
            "3                                                  ['v']\n",
            "4                                   ['manner', 'norton']\n",
            "                             ...                        \n",
            "142                              ['filing', 'situation']\n",
            "143                            ['negative', 'treatment']\n",
            "144    ['negative', 'treatment', 'results', 'situation']\n",
            "145                                          ['history']\n",
            "146    ['history', 'results', 'situation', '5', 'all'...\n",
            "Name: tweet, Length: 147, dtype: object\n",
            "\n",
            "\n",
            "The data is saved into output.csv file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv6fVhOfFmv",
        "outputId": "7f722544-dfdd-46c1-8a42-b34956624ceb"
      },
      "source": [
        "\n",
        "def removeZeros(ip): \n",
        "    new = \".\".join([str(int(i)) for i in ip.split(\".\")])   \n",
        "    return new; \n",
        "ip =\"260.08.094.109\"  \n",
        "print(\"Raw input: 260.08.094.109\")\n",
        "print(\"After removing zeros:\",removeZeros(ip)) \n",
        "  \n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Raw input: 260.08.094.109\n",
            "After removing zeros: 260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdJpDx9gjbX",
        "outputId": "1b47eb77-0c42-4cb1-eefb-548b9836a867"
      },
      "source": [
        "# Write your code here\r\n",
        "# pip install date_extractor\r\n",
        "import re \r\n",
        "import numpy as np\r\n",
        "from date_extractor import extract_dates\r\n",
        "import datetime\r\n",
        "# initializing string  \r\n",
        "test_string = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\r\n",
        "  \r\n",
        "# printing original string  \r\n",
        "#print(\"The original string : \" + test_string) \r\n",
        "print(\"The years in this sentence:\")\r\n",
        "# getting numbers from string  \r\n",
        "res=list(extract_dates(test_string))\r\n",
        "for i in res:\r\n",
        "  print(i.year)\r\n",
        "\r\n"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The years in this sentence:\n",
            "2010\n",
            "2010\n",
            "2019\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}